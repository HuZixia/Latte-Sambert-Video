# Latte-Sambert-Video


## 1. é¡¹ç›®ä»‹ç»

**è¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬è½¬è§†é¢‘çš„é¡¹ç›®ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬ï¼Œç”Ÿæˆå¯¹åº”çš„è§†é¢‘ã€‚**

- æ–‡å­—ç”Ÿæˆè§†é¢‘ï¼ŒLatteï¼Œè¯¦è§ latte_video.ipynb
- æ–‡å­—ç”Ÿæˆä¸ªæ€§åŒ–è¯­éŸ³ï¼ŒSambertï¼Œè¯¦è§ sambert_audio.ipynb
- è§†é¢‘å’Œè¯­éŸ³åˆæˆï¼Œå¸¦æœ‰å­—å¹• ï¼Œè¯¦è§ merge_video.ipynb


## 2. æ•ˆæœå±•ç¤º

è§†é¢‘ç»“æœåœ¨ç›®å½• `data/result` ä¸‹ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªä¾‹å­ã€‚

https://v.qq.com/x/page/b35425m2cb1.html

https://v.qq.com/x/page/q3542cbxmkt.html



https://www.zhihu.com/zvideo/1746593924586672128



## 3. æ ¸å¿ƒæµç¨‹

**è¯¦è§ main.py**

### 3.1 æ–‡å­—ç”Ÿæˆè§†é¢‘ï¼ŒLatteï¼Œè¯¦è§ latte_video.ipynb

**GPUèµ„æºï¼šGPUç¯å¢ƒï¼Œ8æ ¸ 32GB æ˜¾å­˜24Gï¼Œubuntu22.4-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.12.0**

#### 3.1.1. å®‰è£…ä¾èµ–

```
# !pip install timm
# !pip install einops
# !pip install omegaconf
# !pip install diffusers==0.24.0
```


#### 3.1.2. ä¸‹è½½Latteæ¨¡å‹

```
# %cd Latte/models
# !git lfs install
# !git clone https://www.modelscope.cn/AI-ModelScope/Latte.git
```


#### 3.1.3 ä¿®æ”¹é…ç½®æ–‡ä»¶

```
# ä¿®æ”¹é…ç½®æ–‡ä»¶ ./Latte/configs/t2v/t2v_sample.yaml
# ckpt: ./models/Latte/t2v.pt
# save_img_path: "./sample_videos/t2v"
# pretrained_model_path: "./models/Latte/t2v_required_models"
# text_prompt: è‡ªå®šä¹‰ prompt å…·ä½“å†…å®¹
```


#### 3.1.4. æ‰§è¡ŒLatteç”Ÿæˆè§†é¢‘

```
# %cd ./Latte
# !export CUDA_VISIBLE_DEVICES=0
# !export PYTHONPATH=./
# !python sample/sample_t2v.py --config configs/t2v/t2v_sample.yaml
# GPUèµ„æºï¼šGPUç¯å¢ƒï¼Œ8æ ¸ 32GB æ˜¾å­˜24Gï¼Œubuntu22.4-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.12.0
```


#### 3.1.5. æ‹·è´Latteç”Ÿæˆè§†é¢‘

```
import shutil
def list_dir(dir_path):
    files = []
    for file in os.listdir(dir_path):
        file_path = os.path.join(dir_path, file)
        files.append(file_path)
    return files


def copy_file(src_path, dst_path, new_name):
    shutil.copy(src_path, os.path.join(dst_path, new_name))


def sample_video(src_dir_path, dst_dir_path):
    str = "webv-imageio"
    files = list_dir(src_dir_path)
    for file in files:
        if str in file:
            filename = os.path.basename(file)
            ext = os.path.splitext(filename)[1]
            index = filename.split('_')[1]
            new_name = f"{index}.mp4"
            copy_file(file, dst_dir_path, new_name)
```

### 3.2 æ–‡å­—ç”Ÿæˆä¸ªæ€§åŒ–è¯­éŸ³ï¼ŒSambertï¼Œè¯¦è§ sambert_audio.ipynb


#### 3.2.1 å‡†å¤‡è‡ªå·±çš„è¯­éŸ³æ•°æ®ï¼Œæ”¾åˆ° ./data/test_female


#### 3.2.2 å®‰è£…æœ€æ–°ç‰ˆtts-autolabel

```
# è¿è¡Œæ­¤ä»£ç å—å®‰è£…tts-autolabel
# import sys
# !{sys.executable} -m pip install -U tts-autolabel -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
# å¦‚æœç”±äºç½‘ç»œé—®é¢˜å®‰è£…å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨å›½å†…é•œåƒæº, åœ¨Notebookä¸­æ–°å»ºä¸€ä¸ªä»£ç å—ï¼Œè¾“å…¥å¦‚ä¸‹ä»£ç å¹¶è¿è¡Œ
# !{sys.executable} -m pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
```


#### 3.2.3 è¿è¡ŒTTS-AutoLabelè‡ªåŠ¨æ ‡æ³¨

```
# from modelscope.tools import run_auto_label
# input_wav = "./data/test_female/"
# output_data = "./data/output_training_data/"
# ret, report = run_auto_label(input_wav=input_wav, work_dir=output_data, resource_revision="v1.0.7")
```


#### 3.2.4 åŸºäºPTTS-basemodelå¾®è°ƒ

```
# è·å¾—æ ‡æ³¨å¥½çš„è®­ç»ƒæ•°æ®åï¼Œæˆ‘ä»¬è¿›è¡Œæ¨¡å‹å¾®è°ƒ
# from modelscope.metainfo import Trainers
# from modelscope.trainers import build_trainer
# from modelscope.utils.audio.audio_utils import TtsTrainType
# pretrained_model_id = 'damo/speech_personal_sambert-hifigan_nsf_tts_zh-cn_pretrain_16k'
# dataset_id = "./data/output_training_data/"
# pretrain_work_dir = "./data/pretrain_work_dir/"
# # è®­ç»ƒä¿¡æ¯ï¼Œç”¨äºæŒ‡å®šéœ€è¦è®­ç»ƒå“ªä¸ªæˆ–å“ªäº›æ¨¡å‹ï¼Œè¿™é‡Œå±•ç¤ºAMå’ŒVocoderæ¨¡å‹çš†è¿›è¡Œè®­ç»ƒ
# # ç›®å‰æ”¯æŒè®­ç»ƒï¼šTtsTrainType.TRAIN_TYPE_SAMBERT, TtsTrainType.TRAIN_TYPE_VOC
# # è®­ç»ƒSAMBERTä¼šä»¥æ¨¡å‹æœ€æ–°stepä½œä¸ºåŸºç¡€è¿›è¡Œfinetune
# train_info = {
#     TtsTrainType.TRAIN_TYPE_SAMBERT: {  # é…ç½®è®­ç»ƒAMï¼ˆsambertï¼‰æ¨¡å‹
#         'train_steps': 202,  # è®­ç»ƒå¤šå°‘ä¸ªstep
#         'save_interval_steps': 200,  # æ¯è®­ç»ƒå¤šå°‘ä¸ªstepä¿å­˜ä¸€æ¬¡checkpoint
#         'log_interval': 10  # æ¯è®­ç»ƒå¤šå°‘ä¸ªstepæ‰“å°ä¸€æ¬¡è®­ç»ƒæ—¥å¿—
#     }
# }
# # é…ç½®è®­ç»ƒå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†ï¼Œä¸´æ—¶å·¥ä½œç›®å½•å’Œtrain_info
# kwargs = dict(
#     model=pretrained_model_id,  # æŒ‡å®šè¦finetuneçš„æ¨¡å‹
#     model_revision="v1.0.6",
#     work_dir=pretrain_work_dir,  # æŒ‡å®šä¸´æ—¶å·¥ä½œç›®å½•
#     train_dataset=dataset_id,  # æŒ‡å®šæ•°æ®é›†id
#     train_type=train_info  # æŒ‡å®šè¦è®­ç»ƒç±»å‹åŠå‚æ•°
# )
#
# trainer = build_trainer(Trainers.speech_kantts_trainer,
#                         default_args=kwargs)
# trainer.train()
```

#### 3.2.5 ä¸ªæ€§åŒ–è¯­éŸ³åˆæˆ

```
import os
from modelscope.models.audio.tts import SambertHifigan
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

def personal_audio(pretrain_dir, text_sentences, audio_output):
    model_dir = os.path.abspath(pretrain_dir)
    custom_infer_abs = {
        'voice_name':
        'F7',
        'am_ckpt':
        os.path.join(model_dir, 'tmp_am', 'ckpt'),
        'am_config':
        os.path.join(model_dir, 'tmp_am', 'config.yaml'),
        'voc_ckpt':
        os.path.join(model_dir, 'orig_model', 'basemodel_16k', 'hifigan', 'ckpt'),
        'voc_config':
        os.path.join(model_dir, 'orig_model', 'basemodel_16k', 'hifigan',
                 'config.yaml'),
        'audio_config':
        os.path.join(model_dir, 'data', 'audio_config.yaml'),
        'se_file':
        os.path.join(model_dir, 'data', 'se', 'se.npy')
    }
    kwargs = {'custom_ckpt': custom_infer_abs}
    model_id = SambertHifigan(os.path.join(model_dir, "orig_model"), **kwargs)
    inference = pipeline(task=Tasks.text_to_speech, model=model_id)
    model_dir = os.path.abspath(audio_output)

    # éå†æ–‡æœ¬åˆ—è¡¨,ç”Ÿæˆå¹¶ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    for i, text in enumerate(text_sentences):
        output = inference(input=text)
        audio_file = f"{model_dir}/output_{i}.wav"
        with open(audio_file, "wb") as f:
            f.write(output["output_wav"])
        print(f"å·²ä¿å­˜éŸ³é¢‘æ–‡ä»¶: {audio_file}")
```

### 3.3 è§†é¢‘å’Œè¯­éŸ³åˆæˆï¼Œå¸¦æœ‰å­—å¹• ï¼Œè¯¦è§ merge_video.ipynb

```
from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip
import os
from textwrap import wrap


def list_dir(dir_path):
    files = []
    for file in os.listdir(dir_path):
        file_path = os.path.join(dir_path, file)
        files.append(file_path)
    return files


def merge_video(video_dir, audio_dir, output, text_sentences, font='./asserts/SimHei.ttf'):
    video_files = sorted(list_dir(video_dir))
    audio_files = sorted(list_dir(audio_dir))
    final_clips = []
    i = 0
    chars_per_line = 20
    for video_file, audio_file in zip(video_files, audio_files):
        video_clip = VideoFileClip(video_file)
        audio_clip = AudioFileClip(audio_file)
        video_duration = video_clip.duration
        audio_duration = audio_clip.duration
        wrapped_text = "\n".join(wrap(text_sentences[i], chars_per_line))
        txt_clip = TextClip(wrapped_text, fontsize=14, color='white', font=font)
        txt_clip = txt_clip.set_position(('center', 'bottom')).set_duration(audio_clip.duration)
        i += 1
        if video_duration < audio_duration:
            n_loops = audio_duration  # video_duration + 1
            video_clip = video_clip.loop(n=n_loops)
            video_clip = video_clip.subclip(0, audio_duration)

        video_clip = video_clip.set_audio(audio_clip)

        video_clip = CompositeVideoClip([video_clip, txt_clip])

        final_clips.append(video_clip)

    final_video = concatenate_videoclips(final_clips)
    print(output)

    final_video.write_videofile(output, codec='libx264', audio_codec='aac')
```



## 4. ä½œè€…ä¿¡æ¯

`@Author    : RedHerring`

`@å¾®ä¿¡å…¬ä¼—å·  : AI Freedom`

`@çŸ¥ä¹       : RedHerring`

æ¬¢è¿å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼Œå­¦ä¹ äº¤æµ ğŸ¤“

<img src="./asserts/AI Freedom.jpg" style="margin-left: 0px">


